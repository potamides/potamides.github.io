<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning from Self-Feedback for Graphics Program Synthesis | Jonas Belouadi </title> <meta name="author" content="Jonas Belouadi"> <meta name="description" content="Jonas Belouadi's Homepage"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://pvinis.github.io/iosevka-webfont/3.4.1/iosevka.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://potamides.github.io/blog/2025/rlsf/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonas</span> Belouadi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/playground/">Playground </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv.pdf" target="_blank" rel="noopener noreferrer">CV <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning from Self-Feedback for Graphics Program Synthesis</h1> <p class="post-meta"> Created on June 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://github.com/potamides/DeTikZify" rel="external nofollow noopener" target="_blank">DeTi<i>k</i>Zify</a> is a family of multimodal large language models that automatically convert hand-drawn sketches or images of existing scientific figures into editable, semantics-preserving Ti<em>k</em>Z graphics programs. It aims to simplify the process of creating and editing high-quality scientific figures, which traditionally is time-consuming and challenging. Here is an example workflow showing how to interact with the model:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://github.com/potamides/DeTikZify/assets/53401822/203d2853-0b5c-4a2b-9d09-3ccb65880cd3" class="img-fluid rounded z-depth-1 video-16-9" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <h2 id="background">Background</h2> <p>The video also showcases an iterative inference algorithm based on <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="external nofollow noopener" target="_blank">Monte Carlo Tree Search</a> (MCTS), which enables DeTi<em>k</em>Zify to continuously refine its outputs without additional training. The reward scores required by MCTS are computed entirely using DeTi<em>k</em>Zify’s vision encoder, by visually assessing the similarity between input figures and compiled generated outputs. External reward models are not only unnecessary but often have lower correlations with human judgments, as the vision encoder was fine-tuned end-to-end with the entire model, optimizing it for evaluating this specific task. We refer readers to the <a href="https://arxiv.org/abs/2405.15306" rel="external nofollow noopener" target="_blank">DeTi<i>k</i>Zify</a> and <a href="https://arxiv.org/abs/2503.11509" rel="external nofollow noopener" target="_blank">Ti<i>k</i>Zero</a> papers for further details. These self-computed rewards have been effective in enhancing model outputs during <em>inference</em>. With reinforcement learning algorithms like <a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">Group Relative Policy Optimization</a>, this reward signal could also be used for the model to improve itself during a <em>post-training step</em>, i.e., reinforcement learning from self-feedback (RLSF). In this post, we investigate how post-training <a href="https://huggingface.co/nllg/detikzify-v2-8b" rel="external nofollow noopener" target="_blank">DeTi<i>k</i>Zify<sub>v2</sub></a>, the previously state-of-the-art DeTi<em>k</em>Zify model, improves performance. We call the resulting model <a href="https://huggingface.co/nllg/detikzify-v2.5-8b" rel="external nofollow noopener" target="_blank">DeTi<i>k</i>Zify<sub>v2.5</sub></a>.</p> <h2 id="model-training">Model Training</h2> <p>Our post-training setup does only require figures, and does not require aligned code as in supervised fine-tuning, granting us more flexibility in selecting training data. 50% of the training data comes from the subset of <a href="https://huggingface.co/datasets/nllg/datikz-v3" rel="external nofollow noopener" target="_blank">DaTikZ<sub>v3</sub></a>, that was filtered out during the training of DeTi<em>k</em>Zify<sub>v2</sub>. The remaining 50% is sampled from the <a href="https://huggingface.co/datasets/google/spiqa" rel="external nofollow noopener" target="_blank">SPIQA</a> dataset, which contains image labels for figures extracted from <a href="https://arxiv.org" rel="external nofollow noopener" target="_blank">arXiv</a>. We exclude all figures from papers included in DaTikZ<sub>v3</sub>. We sample this split so that 60% of these figures are labeled as schematics, 20% as plots, and 20% come from other categories. Since these figures are not necessarily created from Ti<em>k</em>Z, they may aid in enhancing the model’s generalization capabilities. As with DeTi<em>k</em>Zify<sub>v2</sub>, input figures are randomly converted into synthetic sketches using image transformations and <a href="https://huggingface.co/nllg/ultrasketch" rel="external nofollow noopener" target="_blank">UltraSketch</a>. Using this dataset, we post-train DeTi<em>k</em>Zify<sub>v2</sub> with RLSF, employing a batch size of 16. For each image, 32 outputs are generated, resulting in the model being trained on 512 outputs per step. We train for a total of 500 steps which takes 5 days to complete on eight Nvidia H200 GPUs. We keep the vision encoder frozen to mitigate reward hacking.</p> <h2 id="experiments-and-results">Experiments and Results</h2> <p>We evaluate DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) on the test split of DaTi<em>k</em>Z<sub>v3</sub> and compare it to DeTi<em>k</em>Zify<sub>v2</sub> (8b). The metrics employed include DreamSim (DSim), Kernel Inception Distance (KID), CrystalBLEU (cBLEU), TeX Edit Distance (TED), Mean Token Efficiency (MTE), and Mean Sampling Throughput (MST). Refer to the <a href="https://arxiv.org/abs/2405.15306" rel="external nofollow noopener" target="_blank">DeTi<i>k</i>Zify</a> paper for further details. All scores except MST are multiplied by 100.</p> <h3 id="sampling-based-inference">Sampling-based Inference</h3> <table class="table table-sm"> <caption> Results for sampling-based inference. Arrows indicate metric directionality. Overall, DeTi<i>k</i>Zify<sub>v2.5</sub> demonstrates the best performance. </caption> <thead> <tr> <th></th> <th colspan="5">Reference Figures</th> <th colspan="5">Synthetic Sketches</th> </tr> <tr> <th>Model</th> <th>DSim<sub>↑</sub> </th> <th>KID<sub>↓</sub> </th> <th>cBLEU<sub>↑</sub> </th> <th>TED<sub>↓</sub> </th> <th>MTE<sub>↑</sub> </th> <th>DSim<sub>↑</sub> </th> <th>KID<sub>↓</sub> </th> <th>cBLEU<sub>↑</sub> </th> <th>TED<sub>↓</sub> </th> <th>MTE<sub>↑</sub> </th> </tr> </thead> <tbody> <tr> <td>DeTi<i>k</i>Zify<sub>v2</sub> (8b)</td> <td>80.503</td> <td>0.626</td> <td><b>6.105</b></td> <td>54.946</td> <td>93.326</td> <td>74.584</td> <td>0.751</td> <td><b>3.356</b></td> <td>58.32</td> <td>93.858</td> </tr> <tr> <td>DeTi<i>k</i>Zify<sub>v2.5</sub> (8b)</td> <td><b>84.6438</b></td> <td><b>0.298</b></td> <td>4.202</td> <td><b>52.939</b></td> <td><b>100</b></td> <td><b>78.257</b></td> <td><b>0.577</b></td> <td>1.551</td> <td><b>56.121</b></td> <td><b>100</b></td> </tr> </tbody> </table> <p>In sampling-based inference (i.e., accepting the first output that compiles successfully) using reference figures and synthetic sketch inputs, DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) outperforms DeTi<em>k</em>Zify<sub>v2</sub> (8b) on most metrics, demonstrating that RLSF can effectively enhance performance. The considerably increased DreamSim scores indicate that DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) generates outputs that are much more visually similar to the reference figures. Furthermore, it is much less likely to produce outputs that do not compile, as evidenced by its perfect MTE score. Interestingly, while it scores lower on the code-based metric CrystalBLEU, it performs better on the code-based TED. DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) tends to generate more concise programs with less syntactic noise. While this likely reduces the n-gram overlap with the reference code, it also decreases the number of edits necessary to convert one into another, explaining this phenomenon. Generally, more concise programs are beneficial as long as the semantics are preserved.</p> <h3 id="mcts-based-inference">MCTS-based Inference</h3> <table class="table table-sm"> <caption> Results for MCTS-based inference. Arrows indicate metric directionality. Overall, DeTi<i>k</i>Zify<sub>v2.5</sub> demonstrates the best performance. </caption> <thead> <tr> <th></th> <th colspan="5">Reference Figures</th> <th colspan="5">Synthetic Sketches</th> </tr> <tr> <th>Model</th> <th>DSim<sub>↑</sub> </th> <th>KID<sub>↓</sub> </th> <th>cBLEU<sub>↑</sub> </th> <th>TED<sub>↓</sub> </th> <th>MST<sub>↑</sub> </th> <th>DSim<sub>↑</sub> </th> <th>KID<sub>↓</sub> </th> <th>cBLEU<sub>↑</sub> </th> <th>TED<sub>↓</sub> </th> <th>MST<sub>↑</sub> </th> </tr> </thead> <tbody> <tr> <td>DeTi<i>k</i>Zify<sub>v2</sub> (8b)</td> <td>89.020</td> <td>0.016</td> <td><b>6.593</b></td> <td>52.466</td> <td>52.723</td> <td>81.482</td> <td><b>0.313</b></td> <td><b>3.344</b></td> <td>56.405</td> <td>53.586</td> </tr> <tr> <td>DeTi<i>k</i>Zify<sub>v2.5</sub> (8b)</td> <td><b>90.889</b></td> <td><b>-0.047</b></td> <td>4.646</td> <td><b>51.824</b></td> <td><b>68.12</b></td> <td><b>83.74</b></td> <td>0.61</td> <td>1.976</td> <td><b>55.239</b></td> <td><b>78.908</b></td> </tr> </tbody> </table> <p>We observe similar trends when using our MCTS-based inference algorithm with a time budget of 10 minutes. Compared to sampling-based inference, DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) noticeably improves its scores, illustrating that MCTS on top of RLSF can still lead to additional gains. Additionally, within the same timeframe, DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) generates 25 more outputs than DeTi<em>k</em>Zify<sub>v2</sub> (8b), supporting our hypothesis that the generated programs are more concise. On reference figures, DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) scores better on both DreamSim and KID, with the KID score even being slightly negative due to the high similarity of distributions. For synthetic sketches, it achieves a higher DreamSim score but performs worse on KID, indicating a prioritization of faithfulness to the reference figure over just focusing on general aesthetics.</p> <h3 id="inference-with-tikzero-adapters">Inference with Ti<em>k</em>Zero Adapters</h3> <table class="table table-sm"> <caption> Results for sampling-based inference with Ti<i>k</i>Zero adapters. Arrows indicate metric directionality. </caption> <thead> <tr> <th></th> <th colspan="6">Captions</th> </tr> <tr> <th>Model</th> <th>DSim<sub>↑</sub> </th> <th>KID<sub>↓</sub> </th> <th>CLIP<sub>↑</sub> </th> <th>cBLEU<sub>↑</sub> </th> <th>TED<sub>↓</sub> </th> <th>MTE<sub>↑</sub> </th> </tr> </thead> <tbody> <tr> <td>DeTi<i>k</i>Zify<sub>v2</sub> (8b)</td> <td>52.829</td> <td><b>5.103</b></td> <td><b>10.051</b></td> <td><b>1.603</b></td> <td>65.51</td> <td>82.291</td> </tr> <tr> <td>DeTi<i>k</i>Zify<sub>v2.5</sub> (8b)</td> <td><b>53.564</b></td> <td>7.471</td> <td>7.968</td> <td>0.732</td> <td><b>62.189</b></td> <td><b>100</b></td> </tr> </tbody> </table> <p><a href="https://huggingface.co/nllg/tikzero-adapter" rel="external nofollow noopener" target="_blank">Ti<i>k</i>Zero adapters</a> integrate into the vision encoder of DeTi<em>k</em>Zify models, enabling them to be conditioned on text in addition to images. Since we keep the vision encoder frozen, we can evaluate DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) on adapters trained for DeTi<em>k</em>Zify<sub>v2</sub> (8b). Compared to our previous experiments, the results are more varied. While DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) achieves a better DreamSim value and maintains a perfect MTE, it performs worse on CLIPScore, suggesting difficulties in reproducing text from captions. This could be due to an increased modality gap, as RLSF further refines the model for image-only inputs. We plan to address this in future work by incorporating caption inputs into RLSF training.</p> <h2 id="summary">Summary</h2> <p>Overall, RLSF greatly enhances model performance for most tasks. For image and sketch inputs, DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) emerges as the clear leader. For text inputs via Ti<em>k</em>Zero adapters, the choice between model versions depends on the specific use case, given the trade-offs involved. If you’d like to experiment with the model yourself, feel free to try it in our interactive <a href="playground">playground</a>.</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>This model was trained using computational resources provided by the bwForCluster Helix, as part of the bwHPC-S5 project. The authors acknowledge support from the state of Baden-Württemberg through the bwHPC initiative and the German Research Foundation (DFG) under grant INST 35/1597-1 FUGG. This post was inspired by the paper <a href="https://arxiv.org/abs/2505.20793" rel="external nofollow noopener" target="_blank">Rendering-Aware Reinforcement Learning for Vector Graphics Generation</a> and is based on the DeTi<em>k</em>Zify<sub>v2.5</sub> (8b) <a href="https://huggingface.co/nllg/detikzify-v2.5-8b#model-card-for-detikzifyv25-8b" rel="external nofollow noopener" target="_blank">model card</a>.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jonas Belouadi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?c7ada7c3edc9ae96609b001bfe197324"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>